{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport keras\nimport tensorflow as tf\nimport tensorflow\nimport keras\nimport sklearn\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom platform import  python_version\nfrom glob import glob\nfrom keras import backend as K\nfrom tqdm import tqdm\nfrom PIL import Image\n\n!pip install patchify\nfrom patchify import patchify\n# from PIL import Image\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\nfrom tensorflow.keras.optimizers import Adam\n\nimport shutil\n\n!pip install segmentation-models\nimport segmentation_models as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n%config Completer.use_jedi = False\n\nsm.set_framework('tf.keras')\nsm.framework()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-09T20:04:32.678685Z","iopub.execute_input":"2022-05-09T20:04:32.678951Z","iopub.status.idle":"2022-05-09T20:04:53.511491Z","shell.execute_reply.started":"2022-05-09T20:04:32.678918Z","shell.execute_reply":"2022-05-09T20:04:53.510815Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"base_dir = '/kaggle/input/deepglobe-land-cover-classification-dataset'\nprint(os.listdir(base_dir))\nprint(\"Keras version : \" + keras.__version__)\nprint(\"Tensorflow version : \" + tf.__version__)\nprint(\"Python version : \" + python_version())\nprint(\"Sklearn version : \" + sklearn.__version__)\nprint(\"CV2 version : \" + cv2.__version__)\nprint(\"Pandas version : \" + pd.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:53.513238Z","iopub.execute_input":"2022-05-09T20:04:53.513607Z","iopub.status.idle":"2022-05-09T20:04:53.532459Z","shell.execute_reply.started":"2022-05-09T20:04:53.513574Z","shell.execute_reply":"2022-05-09T20:04:53.531983Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"filenames = pd.read_csv(os.path.join(base_dir, 'metadata.csv'))\nfilenames","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:53.533411Z","iopub.execute_input":"2022-05-09T20:04:53.533731Z","iopub.status.idle":"2022-05-09T20:04:53.564860Z","shell.execute_reply.started":"2022-05-09T20:04:53.533706Z","shell.execute_reply":"2022-05-09T20:04:53.564237Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(filenames['split'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:53.566638Z","iopub.execute_input":"2022-05-09T20:04:53.566979Z","iopub.status.idle":"2022-05-09T20:04:53.577195Z","shell.execute_reply.started":"2022-05-09T20:04:53.566941Z","shell.execute_reply":"2022-05-09T20:04:53.576172Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df = filenames[filenames['split']=='train']\ntrain_df['sat_image_path'] = train_df['sat_image_path'].apply(lambda img_pth: os.path.join(base_dir, img_pth))\ntrain_df['mask_path'] = train_df['mask_path'].apply(lambda img_pth: os.path.join(base_dir, img_pth))\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:53.578503Z","iopub.execute_input":"2022-05-09T20:04:53.579062Z","iopub.status.idle":"2022-05-09T20:04:53.605848Z","shell.execute_reply.started":"2022-05-09T20:04:53.579033Z","shell.execute_reply":"2022-05-09T20:04:53.605152Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"i = np.random.randint(0, len(train_df))\nimg = cv2.imread(train_df['sat_image_path'].iloc[i])\nmask = cv2.imread(train_df['mask_path'].iloc[i])\nplt.figure(figsize=(50,50))\n\nplt.subplot(131)\nplt.axis('off')\nplt.title('Landscape')\nplt.imshow(img)\n\nplt.subplot(132)\nplt.axis('off')\nplt.title('Gray Mask')\nplt.imshow(mask)\n\nplt.subplot(133)\nplt.axis('off')\nplt.title('Gray Mask')\none_hot = cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\nplt.imshow(one_hot, cmap =  'gray')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:53.606753Z","iopub.execute_input":"2022-05-09T20:04:53.607191Z","iopub.status.idle":"2022-05-09T20:04:57.039281Z","shell.execute_reply.started":"2022-05-09T20:04:53.607164Z","shell.execute_reply":"2022-05-09T20:04:57.038033Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class_dict = pd.read_csv(os.path.join(base_dir, 'class_dict.csv'))\nnum_class = len(class_dict)\nprint(f'Number of classes {num_class}')\nclass_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:57.040470Z","iopub.execute_input":"2022-05-09T20:04:57.040679Z","iopub.status.idle":"2022-05-09T20:04:57.058678Z","shell.execute_reply.started":"2022-05-09T20:04:57.040649Z","shell.execute_reply":"2022-05-09T20:04:57.057817Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('masks256') :\n    shutil.rmtree('masks256')\nif os.path.exists('images256') :    \n    shutil.rmtree('images256')\n    \nos.mkdir('images256')\nos.mkdir('masks256')\npatch_size =256\n\n\ndef patches(train_df):\n    num_of_saved_files = 0\n    for img_path, mask_path in tqdm(train_df[['sat_image_path','mask_path']].to_numpy()):\n        image = cv2.imread(img_path, 1)\n        mask = cv2.imread(mask_path)\n        assert image.shape == mask.shape\n        SIZE_X = (image.shape[1]//patch_size)*patch_size \n        SIZE_Y = (image.shape[0]//patch_size)*patch_size\n        \n        image = Image.fromarray(image)\n        image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n        image = np.array(image)             \n\n        patches_img = patchify(image, (256, 256, 3), step=256)  #Step=256 for 256 patches means no overlap\n        patches_mask = patchify(mask, (256, 256, 3), step=256) \n        \n        for i in range(patches_img.shape[0]):\n            for j in range(patches_img.shape[1]):\n                \n                single_patch_mask = patches_mask[i,j,:,:]\n                single_patch_mask = single_patch_mask[0]\n                \n                val, counts = np.unique(single_patch_mask, return_counts=True)\n                max_counts = np.max(counts)/counts.sum()\n                if max_counts < 0.95:\n                    single_patch_img = patches_img[i,j,:,:]\n                    single_patch_img = single_patch_img[0]\n\n                    cv2.imwrite(f'images256/{num_of_saved_files}.tif', single_patch_img)\n                    cv2.imwrite(f'masks256/{num_of_saved_files}.tif', single_patch_mask) \n                    num_of_saved_files += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:04:57.059962Z","iopub.execute_input":"2022-05-09T20:04:57.060153Z","iopub.status.idle":"2022-05-09T20:04:57.073943Z","shell.execute_reply.started":"2022-05-09T20:04:57.060129Z","shell.execute_reply":"2022-05-09T20:04:57.072886Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Run for creating patches 256x256\n# patches(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:05:17.279421Z","iopub.execute_input":"2022-05-09T20:05:17.280348Z","iopub.status.idle":"2022-05-09T20:05:17.283642Z","shell.execute_reply.started":"2022-05-09T20:05:17.280297Z","shell.execute_reply":"2022-05-09T20:05:17.282867Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ncolors = []\nfor (r,g,b) in class_dict[['r', 'g', 'b']].to_numpy():\n    colors.append([r,g,b])\nmap_color = {x:v for x,v in zip(range(num_class),colors)}\ncolors\n\nmap_color","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:47:57.112553Z","iopub.execute_input":"2022-05-09T21:47:57.112812Z","iopub.status.idle":"2022-05-09T21:47:57.123103Z","shell.execute_reply.started":"2022-05-09T21:47:57.112784Z","shell.execute_reply":"2022-05-09T21:47:57.122365Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"{1:[255, 255,0]}","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:48:27.846174Z","iopub.execute_input":"2022-05-09T21:48:27.846562Z","iopub.status.idle":"2022-05-09T21:48:27.854716Z","shell.execute_reply.started":"2022-05-09T21:48:27.846526Z","shell.execute_reply":"2022-05-09T21:48:27.853616Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"markdown","source":"### Preproccesing","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/43884463/how-to-convert-rgb-image-to-one-hot-encoded-3d-array-based-on-color-using-numpy\n# Author : MonsterMax\nnum_classes =7\ndef rgb_to_onehot(rgb_arr):\n    color_dict= map_color\n    num_classes = len(color_dict)\n    shape = rgb_arr.shape[:2]+(num_classes,)\n    arr = np.zeros( shape, dtype=np.int8 )\n    for i, cls_ in enumerate(color_dict):\n        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n    return arr","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:14:55.590673Z","iopub.execute_input":"2022-05-09T20:14:55.590932Z","iopub.status.idle":"2022-05-09T20:14:55.597229Z","shell.execute_reply.started":"2022-05-09T20:14:55.590902Z","shell.execute_reply":"2022-05-09T20:14:55.596082Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ndef preprocess_data_rgb_to_onehot(img, mask, num_class=7):\n    masks = np.zeros((mask.shape[0], 256, 256,7))\n    for i ,m in enumerate(mask):\n        masks[i] =rgb_to_onehot(m)      \n        img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n    return (img,masks)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:14:56.025117Z","iopub.execute_input":"2022-05-09T20:14:56.025384Z","iopub.status.idle":"2022-05-09T20:14:56.032330Z","shell.execute_reply.started":"2022-05-09T20:14:56.025358Z","shell.execute_reply":"2022-05-09T20:14:56.031061Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def onehot_to_rgb(onehot, color_dict= map_color):\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in color_dict.keys():\n        output[single_layer==k] = color_dict[k]\n    return np.uint8(output)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:44:42.992389Z","iopub.execute_input":"2022-05-09T21:44:42.992750Z","iopub.status.idle":"2022-05-09T21:44:42.999818Z","shell.execute_reply.started":"2022-05-09T21:44:42.992711Z","shell.execute_reply":"2022-05-09T21:44:42.999044Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nseed=24\nbatch_size= 32\npatched_dir = '../input/patched/data'\n\ndef data_generator(num_class=7):\n    img_data_gen_args = dict(horizontal_flip=True,\n                      vertical_flip=True,\n                      fill_mode='reflect')\n    \n    image_datagen = ImageDataGenerator(**img_data_gen_args)\n    mask_datagen = ImageDataGenerator(**img_data_gen_args )\n    \n    validation_split= 0.1\n\n    df_len = len(glob(os.path.join(patched_dir,'images256/*.tif')))\n    subset = np.empty(df_len, dtype=object)\n    subset[:int((1-validation_split)*df_len)] = 'train'\n    subset[int((1-validation_split)*df_len):] = 'valid'\n    np.random.shuffle(subset)\n    df_filenames = pd.DataFrame(np.vstack((np.sort(np.array((glob(os.path.join(patched_dir,'images256/*.tif')),glob(os.path.join(patched_dir,'masks256/*.tif'))))), subset)).T, columns = ['img', 'masks','subset'])\n    df_train = df_filenames[df_filenames.subset  == 'train']\n    df_valid = df_filenames[df_filenames.subset  == 'valid']\n\n    image_train_generator = image_datagen.flow_from_dataframe(\n        dataframe = df_train,\n        train_dir=None,\n        x_col=\"img\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None)\n    \n    mask_train_generator = mask_datagen.flow_from_dataframe(\n        dataframe = df_train,\n        train_dir=None,\n        x_col=\"masks\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None)\n    \n    \n\n    mask_valid_generator = image_datagen.flow_from_dataframe(\n        dataframe = df_valid,\n        train_dir=None,\n        x_col=\"masks\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None,\n        validate_filenames=False)\n   \n\n    image_valid_generator = mask_datagen.flow_from_dataframe(\n        dataframe = df_valid,\n        train_dir=None,\n        x_col=\"img\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None,\n        validate_filenames=False) \n     \n    train_generator= zip(image_train_generator,mask_train_generator)\n    valid_generator = zip(image_valid_generator, mask_valid_generator)\n    return(train_generator,valid_generator)\n   \ndef transform_generator(gen):\n    for (img, mask) in gen:\n        before = (img.shape,mask.shape)\n        img, mask = preprocess_data_rgb_to_onehot(img, mask)\n        yield (img, mask)\n        \ndef train_valid_gen(num_class=7):\n    train_gen,valid_gen = data_generator(num_class=num_class)\n    train_gen = transform_generator(train_gen)\n    valid_gen = transform_generator(valid_gen)\n    return (train_gen,valid_gen)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:14:56.629740Z","iopub.execute_input":"2022-05-09T20:14:56.630660Z","iopub.status.idle":"2022-05-09T20:14:56.650203Z","shell.execute_reply.started":"2022-05-09T20:14:56.630619Z","shell.execute_reply":"2022-05-09T20:14:56.649660Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"t, v = train_valid_gen()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:14:57.339634Z","iopub.execute_input":"2022-05-09T20:14:57.340645Z","iopub.status.idle":"2022-05-09T20:14:59.171348Z","shell.execute_reply.started":"2022-05-09T20:14:57.340598Z","shell.execute_reply":"2022-05-09T20:14:59.170489Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"%%time\nx, y=next(t)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:14:59.737749Z","iopub.execute_input":"2022-05-09T20:14:59.738737Z","iopub.status.idle":"2022-05-09T20:15:08.560260Z","shell.execute_reply.started":"2022-05-09T20:14:59.738678Z","shell.execute_reply":"2022-05-09T20:15:08.559565Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"Preprocess_data takes Wall time: 8.82s for batch size = 32","metadata":{}},{"cell_type":"code","source":"def one_hot_encode(label):\n    label_values = colors\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis = -1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1).astype('float')\n\n    return semantic_map","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:15:42.066028Z","iopub.execute_input":"2022-05-09T20:15:42.066294Z","iopub.status.idle":"2022-05-09T20:15:42.071326Z","shell.execute_reply.started":"2022-05-09T20:15:42.066262Z","shell.execute_reply":"2022-05-09T20:15:42.070751Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(imgs, masks):\n    return (imgs,one_hot_encode(masks))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:15:42.357003Z","iopub.execute_input":"2022-05-09T20:15:42.357270Z","iopub.status.idle":"2022-05-09T20:15:42.362132Z","shell.execute_reply.started":"2022-05-09T20:15:42.357240Z","shell.execute_reply":"2022-05-09T20:15:42.361239Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nseed=24\nbatch_size= 32\n\npatched_dir = '../input/patched/data'\n\ndef data_generator(num_class=7):\n    img_data_gen_args = dict(horizontal_flip=True,\n                      vertical_flip=True,\n                      fill_mode='reflect')\n    \n    image_datagen = ImageDataGenerator(**img_data_gen_args, rescale=1.0/255.0)\n    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n\n\n    validation_split= 0.2\n\n    df_len = len(glob(os.path.join(patched_dir,'images256/*.tif')))\n    subset = np.empty(df_len, dtype=object)\n    subset[:int((1-validation_split)*df_len)] = 'train'\n    subset[int((1-validation_split)*df_len):] = 'valid'\n    np.random.shuffle(subset)\n    df_filenames = pd.DataFrame(np.vstack((np.sort(np.array((glob(os.path.join(patched_dir,'images256/*.tif')),glob(os.path.join(patched_dir,'masks256/*.tif'))))), subset)).T, columns = ['img', 'masks','subset'])\n    df_train = df_filenames[df_filenames.subset  == 'train']\n    df_valid = df_filenames[df_filenames.subset  == 'valid']\n\n    image_train_generator = image_datagen.flow_from_dataframe(\n        dataframe = df_train,\n        train_dir=None,\n        x_col=\"img\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None)\n    \n    mask_train_generator = mask_datagen.flow_from_dataframe(\n        dataframe = df_train,\n        train_dir=None,\n        x_col=\"masks\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None)\n    \n\n    mask_valid_generator = mask_datagen.flow_from_dataframe(\n        dataframe = df_valid,\n        train_dir=None,\n        x_col=\"masks\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None)\n   \n\n    image_valid_generator = image_datagen.flow_from_dataframe(\n        dataframe = df_valid,\n        train_dir=None,\n        x_col=\"img\",\n        batch_size= batch_size,\n        seed=24,\n        class_mode = None) \n     \n    train_generator= zip(image_train_generator,mask_train_generator)\n    valid_generator = zip(image_valid_generator, mask_valid_generator)\n    return(train_generator,valid_generator)\n   \ndef transform_generator(gen):\n    for (img, mask) in gen:\n        img, mask = preprocess_data(img, mask)\n        yield (img, mask)\n        \ndef train_valid_gen(num_class=7):\n    train_gen,valid_gen = data_generator(num_class=num_class)\n    train_gen = transform_generator(train_gen)\n    valid_gen = transform_generator(valid_gen)\n    return (train_gen,valid_gen)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:15:42.779563Z","iopub.execute_input":"2022-05-09T20:15:42.779863Z","iopub.status.idle":"2022-05-09T20:15:42.800738Z","shell.execute_reply.started":"2022-05-09T20:15:42.779830Z","shell.execute_reply":"2022-05-09T20:15:42.799751Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"t, v = train_valid_gen()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:15:43.211104Z","iopub.execute_input":"2022-05-09T20:15:43.211352Z","iopub.status.idle":"2022-05-09T20:15:43.591986Z","shell.execute_reply.started":"2022-05-09T20:15:43.211325Z","shell.execute_reply":"2022-05-09T20:15:43.591424Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"%%time\nx, y=next(t)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:15:43.836976Z","iopub.execute_input":"2022-05-09T20:15:43.837976Z","iopub.status.idle":"2022-05-09T20:15:44.915521Z","shell.execute_reply.started":"2022-05-09T20:15:43.837919Z","shell.execute_reply":"2022-05-09T20:15:44.914800Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"one_hot_encode reduces Wall time to 1.11 s","metadata":{}},{"cell_type":"markdown","source":"### Check generator","metadata":{}},{"cell_type":"code","source":"t, v = train_valid_gen()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:18:38.493815Z","iopub.execute_input":"2022-05-09T20:18:38.494767Z","iopub.status.idle":"2022-05-09T20:18:40.528926Z","shell.execute_reply.started":"2022-05-09T20:18:38.494722Z","shell.execute_reply":"2022-05-09T20:18:40.528211Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def check_gens(train_gen, valid_gen):\n    x, y = next(train_gen)\n    i =8\n    plt.figure(figsize = (50,50))\n    plt.subplot(221)\n    plt.axis('off')\n    plt.title('Landscape')\n    plt.imshow(x[i])\n\n    plt.subplot(222)\n    plt.axis('off')\n    plt.title('Gray Mask')\n    mask =  cv2.imread('../input/patched/data/masks256/1.tif')\n    plt.imshow(np.argmax((y)[i],axis=2)*64)\n\n\n    x, y = next(valid_gen)\n    plt.subplot(223)\n    plt.axis('off')\n    plt.title('Landscape')\n    plt.imshow(x[i])\n\n    plt.subplot(224)\n    plt.axis('off')\n    plt.title('Gray Mask')\n    mask =  cv2.imread('../input/patched/data/masks256/1.tif')\n    plt.imshow(np.argmax((y)[i],axis=2)*64)\ncheck_gens(t,v)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:18:52.314430Z","iopub.execute_input":"2022-05-09T20:18:52.314722Z","iopub.status.idle":"2022-05-09T20:18:56.003501Z","shell.execute_reply.started":"2022-05-09T20:18:52.314684Z","shell.execute_reply":"2022-05-09T20:18:56.002515Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n\ndef callback(name):\n    checkpoint = ModelCheckpoint(\"{}.h5\".format(name), monitor='val_iou_score', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n    reduce = ReduceLROnPlateau(monitor='val_iou_score',\n                                   patience=2, \n                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n\n    early = EarlyStopping(monitor=\"val_iou_score\", \n                      mode=\"min\", \n                      patience=2)\n    \n    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(name))\n    \n    callbacks = [checkpoint, early, reduce,tensorboard]","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:21:15.021912Z","iopub.execute_input":"2022-05-09T20:21:15.022228Z","iopub.status.idle":"2022-05-09T20:21:15.030897Z","shell.execute_reply.started":"2022-05-09T20:21:15.022191Z","shell.execute_reply":"2022-05-09T20:21:15.030267Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"### U-net Model","metadata":{}},{"cell_type":"code","source":"\ndef unet(input_size=(256,256,3)):\n    inputs = Input(input_size)\n    \n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(num_class, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:19:59.298917Z","iopub.execute_input":"2022-05-09T20:19:59.299164Z","iopub.status.idle":"2022-05-09T20:19:59.314466Z","shell.execute_reply.started":"2022-05-09T20:19:59.299137Z","shell.execute_reply":"2022-05-09T20:19:59.313901Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"model = unet()\nfrom tensorflow.keras.metrics import MeanIoU\nmodel.compile(optimizer = Adam(lr = 0.0001), loss=sm.losses.categorical_focal_jaccard_loss, metrics=[sm.metrics.iou_score])","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:21:36.603410Z","iopub.execute_input":"2022-05-09T20:21:36.603939Z","iopub.status.idle":"2022-05-09T20:21:37.296222Z","shell.execute_reply.started":"2022-05-09T20:21:36.603909Z","shell.execute_reply":"2022-05-09T20:21:37.295494Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(train_gen,\n                steps_per_epoch=2692// batch_size,\n                epochs=50, validation_data = valid_gen, validation_steps=300//batch_size, callbacks =callbacks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### U-net using segmentation models","metadata":{}},{"cell_type":"code","source":"def u_net_models(backbones):\n    \n    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), 'accuracy']\n    histories = {}\n    \n    for backbone in backbones:\n        callbacks = callback(backbone)\n        \n        model = sm.Unet(backbone, encoder_weights='imagenet', \n                input_shape=(256,256,3),\n                classes=7, activation='softmax')\n        model.compile(optimizer = Adam(lr = 0.0001), loss=sm.losses.categorical_focal_jaccard_loss, metrics=metrics)\n        \n        train_gen,valid_gen = train_valid_gen(7)\n        print('Model : {}'.format(backbone))\n        history = model.fit_generator(train_gen,\n                steps_per_epoch=2692// batch_size,\n                epochs=50, validation_data = valid_gen, validation_steps=300//batch_size, callbacks =callbacks)\n        model.save('{}.h5'.format(backbone))\n        histories[backbone] = history\n    return histories","metadata":{"execution":{"iopub.status.busy":"2022-05-09T18:45:45.287925Z","iopub.status.idle":"2022-05-09T18:45:45.288806Z","shell.execute_reply.started":"2022-05-09T18:45:45.288431Z","shell.execute_reply":"2022-05-09T18:45:45.288465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"backbones = ['resnet34', 'inceptionv3', 'vgg16']","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:29:57.016841Z","iopub.execute_input":"2022-05-09T21:29:57.017092Z","iopub.status.idle":"2022-05-09T21:29:57.020999Z","shell.execute_reply.started":"2022-05-09T21:29:57.017063Z","shell.execute_reply":"2022-05-09T21:29:57.020221Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"# histories = u_net_models(backbones)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T18:45:45.290283Z","iopub.status.idle":"2022-05-09T18:45:45.291087Z","shell.execute_reply.started":"2022-05-09T18:45:45.29075Z","shell.execute_reply":"2022-05-09T18:45:45.290787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_image(model, img):\n    w,h = img.shape[:2]\n    w = w // 256 +1\n    h = h // 256 +1\n    padding_shape = (w*256,h*256 ,3)\n    padded_img= np.zeros(padding_shape)\n    mask_shape = (w*256,h*256,7)\n\n    padded_img[0:img.shape[0], 0:img.shape[1], :] = img\n    padded_img = padded_img/255\n    mask_padded = np.zeros(mask_shape)\n\n    for i in range(0,mask_shape[0], 256):\n        for j in range(0, mask_shape[1], 256):\n            patch = padded_img[i:i+256,j:j+256,:]\n            predicted  = model.predict(np.expand_dims(patch,axis=0))\n            mask_padded[i:i+256,j:j+256,:] =predicted \n    return mask_padded[0:img.shape[0], 0:img.shape[1], :]","metadata":{"execution":{"iopub.status.busy":"2022-05-09T20:26:08.803415Z","iopub.execute_input":"2022-05-09T20:26:08.804264Z","iopub.status.idle":"2022-05-09T20:26:08.812181Z","shell.execute_reply.started":"2022-05-09T20:26:08.804200Z","shell.execute_reply":"2022-05-09T20:26:08.811361Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"models_path = ['../input/models/resnet34.h5','../input/models/inceptionv3.h5', '../input/models/vgg16.h5']\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:00:43.015697Z","iopub.execute_input":"2022-05-09T22:00:43.016007Z","iopub.status.idle":"2022-05-09T22:00:43.120401Z","shell.execute_reply.started":"2022-05-09T22:00:43.015974Z","shell.execute_reply":"2022-05-09T22:00:43.119515Z"},"trusted":true},"execution_count":209,"outputs":[]},{"cell_type":"code","source":"def predict(img, paths):\n    predicted_masks = []\n    resized_masks  = []\n    for path in models_path:\n        model = keras.models.load_model(path,\n                                        custom_objects={'focal_loss_plus_jaccard_loss': sm.losses.categorical_focal_jaccard_loss,\n                                                        'iou_score':sm.metrics.IOUScore, \n                                                        'threshold': 0.5,\n                                                        'f1-score':sm.metrics.FScore})\n\n        predicted_masks.append(predict_image(model, img))\n        resized_masks.append(model.predict(np.expand_dims(resized,axis=0)))\n    return predicted_masks,resized_masks","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:02:25.056096Z","iopub.execute_input":"2022-05-09T22:02:25.056412Z","iopub.status.idle":"2022-05-09T22:02:25.064979Z","shell.execute_reply.started":"2022-05-09T22:02:25.056379Z","shell.execute_reply":"2022-05-09T22:02:25.064146Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('../input/deepglobe-land-cover-classification-dataset/train/10452_sat.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:11:36.891015Z","iopub.execute_input":"2022-05-09T22:11:36.891325Z","iopub.status.idle":"2022-05-09T22:11:36.983499Z","shell.execute_reply.started":"2022-05-09T22:11:36.891288Z","shell.execute_reply":"2022-05-09T22:11:36.982216Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"predicted_masks,resized_masks = predict(img, models_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:11:37.341427Z","iopub.execute_input":"2022-05-09T22:11:37.342297Z","iopub.status.idle":"2022-05-09T22:13:26.112494Z","shell.execute_reply.started":"2022-05-09T22:11:37.342219Z","shell.execute_reply":"2022-05-09T22:13:26.111717Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"def plot(predicted_masks,resized_masks ):\n    plt.figure(figsize=(50,50))\n    plt.subplots_adjust(bottom=0.3, top=0.7, hspace=0)\n\n    plt.subplot(151)\n    plt.title('Original Image' ,fontsize=18)\n    plt.axis('off')\n    plt.imshow(img)\n\n    plt.subplot(152)\n    plt.title('Mask {}'.format(backbones[0]) ,fontsize=18)\n    plt.axis('off')\n    plt.imshow(onehot_to_rgb(predicted_masks[0]))\n\n    plt.subplot(153)\n    plt.title('Mask {}'.format(backbones[1]) ,fontsize=18)\n    plt.axis('off')\n    plt.imshow(onehot_to_rgb(predicted_masks[1]))\n\n    plt.subplot(154)\n    plt.title('Mask {}'.format(backbones[2]) ,fontsize=18)\n    plt.axis('off')\n    plt.imshow(onehot_to_rgb(predicted_masks[2]))\n    plt.savefig('models_masks.jpeg',dpi = 100)\nplot(predicted_masks,resized_masks)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:18:33.176041Z","iopub.execute_input":"2022-05-09T22:18:33.176597Z","iopub.status.idle":"2022-05-09T22:18:43.949935Z","shell.execute_reply.started":"2022-05-09T22:18:33.176560Z","shell.execute_reply":"2022-05-09T22:18:43.948862Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('../input/deepglobe-land-cover-classification-dataset/valid/251219_sat.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:19:26.770284Z","iopub.execute_input":"2022-05-09T22:19:26.770630Z","iopub.status.idle":"2022-05-09T22:19:26.874927Z","shell.execute_reply.started":"2022-05-09T22:19:26.770593Z","shell.execute_reply":"2022-05-09T22:19:26.872948Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"predicted_masks,resized_masks = predict(img, models_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:19:27.221030Z","iopub.execute_input":"2022-05-09T22:19:27.221740Z","iopub.status.idle":"2022-05-09T22:21:17.234860Z","shell.execute_reply.started":"2022-05-09T22:19:27.221708Z","shell.execute_reply":"2022-05-09T22:21:17.234010Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"code","source":"plot(predicted_masks,resized_masks)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:21:17.236396Z","iopub.execute_input":"2022-05-09T22:21:17.236646Z","iopub.status.idle":"2022-05-09T22:21:25.116420Z","shell.execute_reply.started":"2022-05-09T22:21:17.236616Z","shell.execute_reply":"2022-05-09T22:21:25.115602Z"},"trusted":true},"execution_count":228,"outputs":[]}]}